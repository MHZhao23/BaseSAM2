{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c3b1c46-9f5c-41c1-9101-85db8709ec0d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T09:32:30.012709Z",
     "start_time": "2024-09-15T09:32:30.009114Z"
    }
   },
   "outputs": [],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7a0db5-7f04-4845-8b11-684fe6e9f7f2",
   "metadata": {},
   "source": [
    "# Video segmentation with SAM 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ba7875-35e5-478b-b8ba-4b48e121dec7",
   "metadata": {},
   "source": [
    "This notebook shows how to use SAM 2 for interactive segmentation in videos. It will cover the following:\n",
    "\n",
    "- adding clicks on a frame to get and refine _masklets_ (spatio-temporal masks) \n",
    "- propagating clicks to get _masklets_ throughout the video\n",
    "- segmenting and tracking multiple objects at the same time\n",
    "\n",
    "We use the terms _segment_ or _mask_ to refer to the model prediction for an object on a single frame, and _masklet_ to refer to the spatio-temporal masks across the entire video. \n",
    "\n",
    "If running locally using jupyter, first install `segment-anything-2` in your environment using the [installation instructions](https://github.com/facebookresearch/segment-anything-2#installation) in the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5318a85-5bf7-4880-b2b3-15e4db24d796",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T03:26:05.253501Z",
     "start_time": "2025-03-10T03:26:03.651329Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08ba49d8-8c22-4eba-a2ab-46eee839287f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T03:26:11.115253Z",
     "start_time": "2025-03-10T03:26:11.110686Z"
    }
   },
   "outputs": [],
   "source": [
    "# use bfloat16 for the entire notebook\n",
    "torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16).__enter__()\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=\"1\"\n",
    "\n",
    "if torch.cuda.get_device_properties(0).major >= 8:\n",
    "    # turn on tfloat32 for Ampere GPUs (https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices)\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8e0779-751f-4224-9b04-ed0f0b406500",
   "metadata": {},
   "source": [
    "### Loading the SAM 2 video predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b9e5d6f97cdcfd9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T03:26:15.754718Z",
     "start_time": "2025-03-10T03:26:15.749807Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cu121\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5f3245e-b4d6-418b-a42a-a67e0b3b5aec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T03:27:01.866666Z",
     "start_time": "2025-03-10T03:27:00.843115Z"
    }
   },
   "outputs": [],
   "source": [
    "from sam2.build_sam import build_sam2_video_predictor\n",
    "\n",
    "# sam2_checkpoint = \"./checkpoints/sam2.1_hiera_small.pt\"\n",
    "# sam2_checkpoint = \"./checkpoints/sam2.1_hiera_s_ioct.pt\"\n",
    "sam2_checkpoint = \"./sam2_logs/configs/sam2.1_training/sam2.1_hiera_s_ioct/checkpoints/checkpoint.pt\"\n",
    "\n",
    "model_cfg = \"configs/sam2.1/sam2.1_hiera_s.yaml\"\n",
    "\n",
    "predictor = build_sam2_video_predictor(model_cfg, sam2_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a5320fe-06d7-45b8-b888-ae00799d07fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T03:27:03.601826Z",
     "start_time": "2025-03-10T03:27:03.595747Z"
    }
   },
   "outputs": [],
   "source": [
    "def show_mask(mask, ax, obj_id=None, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        cmap = plt.get_cmap(\"tab10\")\n",
    "        cmap_idx = 0 if obj_id is None else obj_id\n",
    "        color = np.array([*cmap(cmap_idx)[:3], 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "\n",
    "def show_points(coords, labels, ax, marker_size=200):\n",
    "    pos_points = coords[labels==1]\n",
    "    neg_points = coords[labels==0]\n",
    "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a65e8ba",
   "metadata": {},
   "source": [
    "### Select an example video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fa4004f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame loading (JPEG): 100%|██████████| 1/1 [00:00<00:00, 99.79it/s]\n"
     ]
    }
   ],
   "source": [
    "video_name = 'OD-2025-01-14_153544_test_1_cropped' # 'OD-2024-11-26_151419_test_1_cropped' 'OD-2025-01-14_153544_test_1_cropped' \n",
    "video_path = f\"../data/videos/{video_name}.mp4\"\n",
    "support_dir =\"../data/SUP\"\n",
    "support_frame_dir = os.path.join(support_dir, 'JPEGImages')\n",
    "support_mask_dir = os.path.join(support_dir, 'Annotations')\n",
    "inference_state = predictor.init_state(video_path=video_path, support_path=support_frame_dir)\n",
    "predictor.reset_state(inference_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac09ee5",
   "metadata": {},
   "source": [
    "#### Method 1: Add support masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1606c484",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Code\\work\\ioctVideo\\iOCTSAM2\\sam2\\sam2_video_predictor.py:1012: UserWarning: cannot import name '_C' from 'sam2' (d:\\Code\\work\\ioctVideo\\iOCTSAM2\\sam2\\__init__.py)\n",
      "\n",
      "Skipping the post-processing step due to the error above. You can still use SAM 2 and it's OK to ignore the error above, although some post-processing functionality may be limited (which doesn't affect the results in most cases; see https://github.com/facebookresearch/sam2/blob/main/INSTALL.md).\n",
      "  pred_masks_gpu = fill_holes_in_mask_scores(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding mask from support frame 0 for frame 0 as input for object_id=1\n",
      "adding mask from support frame 0 for frame 0 as input for object_id=2\n",
      "adding mask from support frame 0 for frame 0 as input for object_id=3\n",
      "adding mask from support frame 0 for frame 50 as input for object_id=1\n",
      "adding mask from support frame 0 for frame 50 as input for object_id=2\n",
      "adding mask from support frame 0 for frame 50 as input for object_id=3\n",
      "adding mask from support frame 0 for frame 100 as input for object_id=1\n",
      "adding mask from support frame 0 for frame 100 as input for object_id=2\n",
      "adding mask from support frame 0 for frame 100 as input for object_id=3\n",
      "adding mask from support frame 0 for frame 150 as input for object_id=1\n",
      "adding mask from support frame 0 for frame 150 as input for object_id=2\n",
      "adding mask from support frame 0 for frame 150 as input for object_id=3\n",
      "adding mask from support frame 0 for frame 200 as input for object_id=1\n",
      "adding mask from support frame 0 for frame 200 as input for object_id=2\n",
      "adding mask from support frame 0 for frame 200 as input for object_id=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "propagate in video: 100%|██████████| 643/643 [00:35<00:00, 18.32it/s]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def load_ann_png(path):\n",
    "    \"\"\"Load a PNG file as a mask and its palette.\"\"\"\n",
    "    mask = Image.open(path)\n",
    "    palette = mask.getpalette()\n",
    "    mask = np.array(mask).astype(np.uint8)\n",
    "    return mask, palette\n",
    "\n",
    "def get_per_obj_mask(mask):\n",
    "    \"\"\"Split a mask into per-object masks.\"\"\"\n",
    "    object_ids = np.unique(mask)\n",
    "    object_ids = object_ids[object_ids > 0].tolist()\n",
    "    per_obj_mask = {object_id: (mask == object_id) for object_id in object_ids}\n",
    "    return per_obj_mask\n",
    "\n",
    "def load_masks_from_dir(\n",
    "    input_mask_dir, frame_name, allow_missing=False\n",
    "):\n",
    "    \"\"\"Load masks from a directory as a dict of per-object masks.\"\"\"\n",
    "    input_mask_path = os.path.join(input_mask_dir, frame_name)\n",
    "    if not os.path.exists(input_mask_path):\n",
    "        pass\n",
    "    if allow_missing and not os.path.exists(input_mask_path):\n",
    "        return {}, None\n",
    "    input_mask, input_palette = load_ann_png(input_mask_path)\n",
    "    per_obj_input_mask = get_per_obj_mask(input_mask)\n",
    "\n",
    "    return per_obj_input_mask, input_palette\n",
    "\n",
    "\n",
    "# collect all the object ids and the support set\n",
    "use_all_masks=False\n",
    "inputs_per_object = defaultdict(dict)\n",
    "support_mask_dir = os.path.join(support_dir, 'Annotations')\n",
    "video_segments = defaultdict(dict)\n",
    "if os.path.exists(support_mask_dir):\n",
    "    sup_frame_names = os.listdir(support_mask_dir)\n",
    "    for idx, name in enumerate(sup_frame_names):\n",
    "        per_obj_input_mask, input_palette = load_masks_from_dir(\n",
    "            input_mask_dir=support_mask_dir,\n",
    "            frame_name=name,\n",
    "            allow_missing=False,\n",
    "        )\n",
    "        for object_id, object_mask in per_obj_input_mask.items():\n",
    "            # skip empty masks\n",
    "            if not np.any(object_mask):\n",
    "                continue\n",
    "            # if `use_all_masks=False`, we only use the first mask for each object\n",
    "            if len(inputs_per_object[object_id]) > 0 and not use_all_masks:\n",
    "                continue\n",
    "            inputs_per_object[object_id][idx] = object_mask\n",
    "\n",
    "anno_frame_ids = [0, 50, 100, 150, 200]\n",
    "object_ids = sorted(inputs_per_object)\n",
    "predictor.reset_state(inference_state)\n",
    "\n",
    "for anno_frame_id in anno_frame_ids:\n",
    "    for object_id in object_ids:\n",
    "        # add those input masks to SAM 2 inference state before propagation\n",
    "        input_frame_inds = sorted(inputs_per_object[object_id])\n",
    "        # predictor.reset_state(inference_state)\n",
    "        for input_frame_idx in input_frame_inds:\n",
    "            predictor.add_sup_mask(\n",
    "                inference_state=inference_state,\n",
    "                frame_idx=anno_frame_id,\n",
    "                obj_id=object_id,\n",
    "                sup_frame_idx=input_frame_idx,\n",
    "                sup_mask=inputs_per_object[object_id][input_frame_idx],\n",
    "            )\n",
    "            print(f\"adding mask from support frame {input_frame_idx} for frame {anno_frame_id} as input for {object_id=}\")\n",
    "\n",
    "for out_frame_idx, out_obj_ids, out_mask_logits in predictor.propagate_in_video(inference_state):\n",
    "    c, _, h, w = out_mask_logits.shape\n",
    "    output_mask = np.zeros((1, h, w), dtype=int)\n",
    "    for i, out_obj_id in enumerate(out_obj_ids):\n",
    "        output_mask[(out_mask_logits[i] > 0.0).cpu()] = out_obj_id\n",
    "    output_mask = np.squeeze(output_mask)\n",
    "    video_segments[out_frame_idx] = output_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b3bf4de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame 0\n",
      "frame 50\n",
      "frame 100\n",
      "frame 150\n",
      "frame 200\n",
      "frame 250\n",
      "frame 300\n",
      "frame 350\n",
      "frame 400\n",
      "frame 450\n",
      "frame 500\n",
      "frame 550\n",
      "frame 600\n"
     ]
    }
   ],
   "source": [
    "plot_dir = f'./results/demo/offline_fs/{video_name}'\n",
    "os.makedirs(plot_dir, exist_ok=True)\n",
    "\n",
    "cmap = plt.get_cmap(\"tab10\")\n",
    "class_to_color = {\n",
    "    1: cmap(1),\n",
    "    2: cmap(2),\n",
    "    3: cmap(3),\n",
    "}\n",
    "\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "ret, frame = cap.read()\n",
    "width, height = frame.shape[:2][::-1]\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(plot_dir + \"/output.mp4\", fourcc, 30.0, (width, height))\n",
    "\n",
    "# render the segmentation results every few frames\n",
    "out_frame_idx = 0\n",
    "vis_gap = 50\n",
    "plt.close(\"all\")\n",
    "plt.figure(figsize=(6, 3))\n",
    "while True:\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    width, height = frame.shape[:2][::-1]\n",
    "\n",
    "    if (out_frame_idx % vis_gap) == 0: print(f\"frame {out_frame_idx}\")\n",
    "    output_mask = video_segments[out_frame_idx]\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    colored_mask = np.zeros((output_mask.shape[0], output_mask.shape[1], 4))\n",
    "    for class_id, color in class_to_color.items():\n",
    "        colored_mask[output_mask == class_id] = color  # Assign the corresponding color from the colormap\n",
    "    plt.figure(figsize=(6, 3))\n",
    "    plt.imshow(frame)\n",
    "    plt.imshow(colored_mask, alpha=0.6)  # Adjust alpha to control transparency\n",
    "    plt.axis('off')  # Turn off axis for cleaner display\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(plot_dir, str(out_frame_idx) + '.png'), dpi=100)\n",
    "    plt.close(\"all\")\n",
    "\n",
    "    overlay = 0.6 * frame.astype(np.float32) / 255.0 + 0.6 * colored_mask[:, :, :3]\n",
    "    overlay = np.clip(overlay, 0, 1)\n",
    "    overlay = (overlay * 255).astype(np.uint8)\n",
    "\n",
    "    out.write(cv2.cvtColor(overlay, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "    out_frame_idx += 1\n",
    "\n",
    "cap.release()\n",
    "out.release()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22aa751-b7cd-451e-9ded-fb98bf4bdfad",
   "metadata": {},
   "source": [
    "### Select the testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94c87ca-fd1a-4011-9609-e8be1cbe3230",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T03:27:38.604429Z",
     "start_time": "2025-03-10T03:27:38.134703Z"
    }
   },
   "outputs": [],
   "source": [
    "# `video_dir` a directory of JPEG frames with filenames like `<frame_index>.jpg`\n",
    "# video_dir = \"./datasets/iOCT_lSNR/valid/JPEGImages/seq_1\"\n",
    "video_dir = \"./datasets/videos/OD-2025-01-14_153544_test_1_demo\"\n",
    "support_dir =\"./datasets/iOCT_lSNR/valid/SUP\"\n",
    "support_frame_dir = os.path.join(support_dir, 'JPEGImages')\n",
    "support_mask_dir = os.path.join(support_dir, 'Annotations')\n",
    "\n",
    "# scan all the JPEG frame names in this directory\n",
    "frame_names = [\n",
    "    p for p in os.listdir(video_dir)\n",
    "    if os.path.splitext(p)[-1] in [\".jpg\", \".jpeg\", \".JPG\", \".JPEG\", '.png']\n",
    "]\n",
    "frame_names.sort(key=lambda p: int(os.path.splitext(p)[0]))\n",
    "\n",
    "# take a look the first video frame\n",
    "frame_idx = 0\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.title(f\"frame {frame_idx}\")\n",
    "plt.imshow(Image.open(os.path.join(video_dir, frame_names[frame_idx])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8967aed3-eb82-4866-b8df-0f4743255c2c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T03:27:48.530249Z",
     "start_time": "2025-03-10T03:27:42.291935Z"
    }
   },
   "outputs": [],
   "source": [
    "inference_state = predictor.init_state(video_path=video_dir, support_path=support_frame_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42096b82",
   "metadata": {},
   "source": [
    "#### Method 1: Add support masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d86e70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def load_ann_png(path):\n",
    "    \"\"\"Load a PNG file as a mask and its palette.\"\"\"\n",
    "    mask = Image.open(path)\n",
    "    palette = mask.getpalette()\n",
    "    mask = np.array(mask).astype(np.uint8)\n",
    "    return mask, palette\n",
    "\n",
    "def get_per_obj_mask(mask):\n",
    "    \"\"\"Split a mask into per-object masks.\"\"\"\n",
    "    object_ids = np.unique(mask)\n",
    "    object_ids = object_ids[object_ids > 0].tolist()\n",
    "    per_obj_mask = {object_id: (mask == object_id) for object_id in object_ids}\n",
    "    return per_obj_mask\n",
    "\n",
    "def load_masks_from_dir(\n",
    "    input_mask_dir, frame_name, per_obj_png_file, allow_missing=False\n",
    "):\n",
    "    \"\"\"Load masks from a directory as a dict of per-object masks.\"\"\"\n",
    "    input_mask_path = os.path.join(input_mask_dir, frame_name)\n",
    "    if not os.path.exists(input_mask_path):\n",
    "        pass\n",
    "    if allow_missing and not os.path.exists(input_mask_path):\n",
    "        return {}, None\n",
    "    input_mask, input_palette = load_ann_png(input_mask_path)\n",
    "    per_obj_input_mask = get_per_obj_mask(input_mask)\n",
    "\n",
    "    return per_obj_input_mask, input_palette\n",
    "\n",
    "\n",
    "# collect all the object ids and the support set\n",
    "use_all_masks=False\n",
    "inputs_per_object = defaultdict(dict)\n",
    "support_mask_dir = os.path.join(support_dir, 'Annotations')\n",
    "video_segments = defaultdict(dict)\n",
    "if os.path.exists(support_mask_dir):\n",
    "    sup_frame_names = os.listdir(support_mask_dir)\n",
    "    for idx, name in enumerate(sup_frame_names):\n",
    "        per_obj_input_mask, input_palette = load_masks_from_dir(\n",
    "            input_mask_dir=support_mask_dir,\n",
    "            frame_name=name,\n",
    "            per_obj_png_file=False,  # our dataset combines all object masks into a single PNG file\n",
    "            allow_missing=False,\n",
    "        )\n",
    "        for object_id, object_mask in per_obj_input_mask.items():\n",
    "            # skip empty masks\n",
    "            if not np.any(object_mask):\n",
    "                continue\n",
    "            # if `use_all_masks=False`, we only use the first mask for each object\n",
    "            if len(inputs_per_object[object_id]) > 0 and not use_all_masks:\n",
    "                continue\n",
    "            inputs_per_object[object_id][idx] = object_mask\n",
    "\n",
    "anno_frame_ids = [0, 50, 100]\n",
    "object_ids = sorted(inputs_per_object)\n",
    "predictor.reset_state(inference_state)\n",
    "\n",
    "for anno_frame_id in anno_frame_ids:\n",
    "    for object_id in object_ids:\n",
    "        # add those input masks to SAM 2 inference state before propagation\n",
    "        input_frame_inds = sorted(inputs_per_object[object_id])\n",
    "        # predictor.reset_state(inference_state)\n",
    "        for input_frame_idx in input_frame_inds:\n",
    "            predictor.add_sup_mask(\n",
    "                inference_state=inference_state,\n",
    "                frame_idx=anno_frame_id,\n",
    "                obj_id=object_id,\n",
    "                sup_frame_idx=input_frame_idx,\n",
    "                sup_mask=inputs_per_object[object_id][input_frame_idx],\n",
    "            )\n",
    "            print(f\"adding mask from support frame {input_frame_idx} for frame {anno_frame_id} as input for {object_id=}\")\n",
    "\n",
    "for out_frame_idx, out_obj_ids, out_mask_logits in predictor.propagate_in_video(inference_state):\n",
    "    c, _, h, w = out_mask_logits.shape\n",
    "    output_mask = np.zeros((1, h, w), dtype=int)\n",
    "    for i, out_obj_id in enumerate(out_obj_ids):\n",
    "        output_mask[(out_mask_logits[i] > 0.0).cpu()] = out_obj_id\n",
    "    output_mask = np.squeeze(output_mask)\n",
    "    video_segments[out_frame_idx] = output_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc505c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_dir = './results/demo/offline_sup/lsnr_seq_1'\n",
    "plot_dir = './results/demo/offline_sup/OD-2025-01-14_153544_test_1'\n",
    "os.makedirs(plot_dir, exist_ok=True)\n",
    "\n",
    "cmap = plt.get_cmap(\"tab10\")\n",
    "class_to_color = {\n",
    "    1: cmap(1),\n",
    "    2: cmap(2),\n",
    "    3: cmap(3),\n",
    "}\n",
    "\n",
    "# render the segmentation results every few frames\n",
    "vis_frame_stride = 1\n",
    "plt.close(\"all\")\n",
    "for out_frame_idx in tqdm(range(0, len(frame_names), vis_frame_stride)):\n",
    "    output_mask = video_segments[out_frame_idx]\n",
    "\n",
    "    colored_mask = np.zeros((output_mask.shape[0], output_mask.shape[1], 4))\n",
    "    for class_id, color in class_to_color.items():\n",
    "        colored_mask[output_mask == class_id] = color  # Assign the corresponding color from the colormap\n",
    "    raw_image = np.array(Image.open(os.path.join(video_dir, frame_names[out_frame_idx])))\n",
    "    plt.figure(figsize=(6, 3))\n",
    "    plt.imshow(raw_image)\n",
    "    plt.imshow(colored_mask, alpha=0.6)  # Adjust alpha to control transparency\n",
    "    plt.axis('off')  # Turn off axis for cleaner display\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(plot_dir, frame_names[out_frame_idx]), dpi=100) if vis_frame_stride==1 else plt.show() \n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c63f703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_dir = './results/demo/offline_sup'\n",
    "# os.makedirs(plot_dir, exist_ok=True)\n",
    "\n",
    "# cmap = plt.get_cmap(\"tab10\")\n",
    "# class_to_color = {\n",
    "#     1: cmap(1),\n",
    "#     2: cmap(2),\n",
    "#     3: cmap(3),\n",
    "# }\n",
    "\n",
    "# # render the segmentation results every few frames\n",
    "# vis_frame_stride = 1\n",
    "# plt.close(\"all\")\n",
    "# plt.figure(figsize=(6, 3))\n",
    "# for out_frame_idx in range(0, len(frame_names), vis_frame_stride):\n",
    "#     # output_mask = video_segments[out_frame_idx][2]\n",
    "#     output_mask = np.zeros_like(video_segments[out_frame_idx][1])\n",
    "#     for object_id in object_ids:\n",
    "#         current_mask = video_segments[out_frame_idx][object_id]\n",
    "#         output_mask[current_mask == object_id] = object_id\n",
    "\n",
    "#     colored_mask = np.zeros((output_mask.shape[0], output_mask.shape[1], 4))\n",
    "#     for class_id, color in class_to_color.items():\n",
    "#         colored_mask[output_mask == class_id] = color  # Assign the corresponding color from the colormap\n",
    "#     raw_image = np.array(Image.open(os.path.join(video_dir, frame_names[out_frame_idx])))\n",
    "#     plt.imshow(raw_image)\n",
    "#     plt.imshow(colored_mask, alpha=0.6)  # Adjust alpha to control transparency\n",
    "#     plt.axis('off')  # Turn off axis for cleaner display\n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig(os.path.join(plot_dir, frame_names[out_frame_idx]), dpi=100) if vis_frame_stride==1 else plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26aeb04d-8cba-4f57-95da-6e5a1796003e",
   "metadata": {},
   "source": [
    "#### Method 2: Add a first click on a frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695c7749-b523-4691-aad0-7558c5d1d68c",
   "metadata": {},
   "source": [
    "To get started, let's try to segment the child on the left.\n",
    "\n",
    "Here we make a **positive click** at (x, y) = (210, 350) with label `1`, by sending their coordinates and labels into the `add_new_points` API.\n",
    "\n",
    "Note: label `1` indicates a *positive click (to add a region)* while label `0` indicates a *negative click (to remove a region)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fc4741",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.reset_state(inference_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e749bab-0f36-4173-bf8d-0c20cd5214b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T03:27:54.131782Z",
     "start_time": "2025-03-10T03:27:53.175089Z"
    }
   },
   "outputs": [],
   "source": [
    "prompts = {}\n",
    "ann_frame_idx = 0  # the frame index we interact with\n",
    "\n",
    "\n",
    "# ----------------- annotate the tissue -----------------\n",
    "ann_obj_id = 1\n",
    "points = np.array([[100, 200], [150, 240]], dtype=np.float32)\n",
    "labels = np.array([1, 0], np.int32)\n",
    "\n",
    "prompts[ann_obj_id] = points, labels\n",
    "\n",
    "_, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n",
    "    inference_state=inference_state,\n",
    "    frame_idx=ann_frame_idx,\n",
    "    obj_id=ann_obj_id,\n",
    "    points=points,\n",
    "    labels=labels,\n",
    ")\n",
    "\n",
    "# ----------------- annotate the tool -----------------\n",
    "ann_obj_id = 2\n",
    "points = np.array([[262, 120], [282, 137], [356, 19]], dtype=np.float32)\n",
    "labels = np.array([1, 1, 1], np.int32)\n",
    "\n",
    "prompts[ann_obj_id] = points, labels\n",
    "\n",
    "_, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n",
    "    inference_state=inference_state,\n",
    "    frame_idx=ann_frame_idx,\n",
    "    obj_id=ann_obj_id,\n",
    "    points=points,\n",
    "    labels=labels,\n",
    ")\n",
    "\n",
    "# ----------------- annotate artifacts -----------------\n",
    "ann_obj_id = 3\n",
    "points = np.array([[414, 7], [520, 95], [275, 147], [374, 3]], dtype=np.float32)\n",
    "labels = np.array([1, 1, 1, 0], np.int32)\n",
    "\n",
    "prompts[ann_obj_id] = points, labels\n",
    "\n",
    "_, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n",
    "    inference_state=inference_state,\n",
    "    frame_idx=ann_frame_idx,\n",
    "    obj_id=ann_obj_id,\n",
    "    points=points,\n",
    "    labels=labels,\n",
    ")\n",
    "\n",
    "# ----------------- show the results on the first frame -----------------\n",
    "# show the results on the current (interacted) frame on all objects\n",
    "fig_size = (10, 3)\n",
    "fig, axs = plt.subplots(1, 2, figsize=fig_size)\n",
    "\n",
    "axs[0].imshow(Image.open(os.path.join(video_dir, frame_names[frame_idx])))\n",
    "\n",
    "axs[1].imshow(Image.open(os.path.join(video_dir, frame_names[ann_frame_idx])))\n",
    "show_points(points, labels, plt.gca())\n",
    "for i, out_obj_id in enumerate(out_obj_ids):\n",
    "    show_points(*prompts[out_obj_id], plt.gca())\n",
    "    show_mask((out_mask_logits[i] > 0.0).cpu().numpy(), plt.gca(), obj_id=out_obj_id)\n",
    "\n",
    "fig.suptitle(f\"{frame_names[frame_idx]}\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52015ac-1b7b-4c59-bca3-c2b28484cf46",
   "metadata": {},
   "source": [
    "Propagate the prompts to get the masklet across the video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b025bd-cd58-4bfb-9572-c8d2fd0a02ef",
   "metadata": {},
   "source": [
    "To get the masklet throughout the entire video, we propagate the prompts using the `propagate_in_video` API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab45e932-b0d5-4983-9718-6ee77d1ac31b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T03:28:05.156843Z",
     "start_time": "2025-03-10T03:27:57.026250Z"
    }
   },
   "outputs": [],
   "source": [
    "# run propagation throughout the video and collect the results in a dict\n",
    "video_segments = {}  # video_segments contains the per-frame segmentation results\n",
    "for out_frame_idx, out_obj_ids, out_mask_logits in predictor.propagate_in_video(inference_state):\n",
    "    c, _, h, w = out_mask_logits.shape\n",
    "    output_mask = np.zeros((1, h, w), dtype=int)\n",
    "    for i, out_obj_id in enumerate(out_obj_ids):\n",
    "        output_mask[(out_mask_logits[i] > 0.0).cpu()] = out_obj_id\n",
    "    output_mask = np.squeeze(output_mask)\n",
    "    video_segments[out_frame_idx] = output_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92a8d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dir = './results/demo/offline_sup'\n",
    "os.makedirs(plot_dir, exist_ok=True)\n",
    "\n",
    "cmap = plt.get_cmap(\"tab10\")\n",
    "class_to_color = {\n",
    "    1: cmap(1),\n",
    "    2: cmap(2),\n",
    "    3: cmap(3),\n",
    "}\n",
    "\n",
    "# render the segmentation results every few frames\n",
    "vis_frame_stride = 1\n",
    "plt.close(\"all\")\n",
    "plt.figure(figsize=(6, 3))\n",
    "for out_frame_idx in range(0, len(frame_names), vis_frame_stride):\n",
    "    output_mask = video_segments[out_frame_idx]\n",
    "\n",
    "    colored_mask = np.zeros((output_mask.shape[0], output_mask.shape[1], 4))\n",
    "    for class_id, color in class_to_color.items():\n",
    "        colored_mask[output_mask == class_id] = color  # Assign the corresponding color from the colormap\n",
    "    raw_image = np.array(Image.open(os.path.join(video_dir, frame_names[out_frame_idx])))\n",
    "    plt.imshow(raw_image)\n",
    "    plt.imshow(colored_mask, alpha=0.6)  # Adjust alpha to control transparency\n",
    "    plt.axis('off')  # Turn off axis for cleaner display\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(plot_dir, frame_names[out_frame_idx]), dpi=100) if vis_frame_stride==1 else plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9348e2b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ioctsam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
